# ü§ñ Chatbot SECS/UFAL - Arquitetura T√©cnica

**Sistema RAG com HyDE para Consulta de Documentos Institucionais**

**Vers√£o**: 7.1  
**Data**: 06/12/2025  
**Autor**: F√°bio Linhares

---

## üìã Sum√°rio Executivo

Sistema de Retrieval-Augmented Generation (RAG) desenvolvido para facilitar o acesso a documentos institucionais da UFAL. Combina busca sem√¢ntica vetorial, LLMs via OpenRouter, e HyDE (Hypothetical Document Embeddings) para respostas precisas sobre regimentos, resolu√ß√µes, atas e pautas dos Conselhos Superiores.

**Caracter√≠sticas**:
- üîç Busca sem√¢ntica com embeddings OpenRouter (1536 dims)
- üß† HyDE para +20-30% precis√£o
- üîê Permiss√µes granulares (global/privado)
- üì§ Upload autom√°tico de PDFs
- üíæ Cache multin√≠vel (98% redu√ß√£o lat√™ncia)
- üéØ Agentes especializados

**Otimizado para hardware modesto** (Celeron N3050, 8GB RAM)

---

## 1. Arquitetura do Sistema

### 1.1 Vis√£o Geral

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    INTERFACE (Streamlit)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   CAMADA DE APLICA√á√ÉO                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ Focal Agent  ‚îÇ  ‚îÇ Semantic     ‚îÇ  ‚îÇ HyDE Query   ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ (7 tools)    ‚îÇ  ‚îÇ Rewriter     ‚îÇ  ‚îÇ Expander     ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PIPELINE RAG                             ‚îÇ
‚îÇ  1. Query Enhancement (Semantic Rewriter)                   ‚îÇ
‚îÇ  2. HyDE (opcional) - Gera resposta hipot√©tica              ‚îÇ
‚îÇ  3. Vector Search - Busca top-k chunks                      ‚îÇ
‚îÇ  4. Context Building - Monta prompt com fontes              ‚îÇ
‚îÇ  5. LLM Generation - Gera resposta                          ‚îÇ
‚îÇ  6. Source Citation - Cita documentos                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚ñº                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  VECTOR STORE    ‚îÇ    ‚îÇ   LLM SERVICE    ‚îÇ
‚îÇ  (SQLite)        ‚îÇ    ‚îÇ  (OpenRouter)    ‚îÇ
‚îÇ                  ‚îÇ    ‚îÇ                  ‚îÇ
‚îÇ  ‚Ä¢ Embeddings    ‚îÇ    ‚îÇ  ‚Ä¢ Gemini Flash  ‚îÇ
‚îÇ  ‚Ä¢ Similarity    ‚îÇ    ‚îÇ  ‚Ä¢ Streaming     ‚îÇ
‚îÇ  ‚Ä¢ Permissions   ‚îÇ    ‚îÇ  ‚Ä¢ Temperature   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ EMBEDDING API    ‚îÇ
‚îÇ OpenRouter       ‚îÇ
‚îÇ (1536 dims)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.2 Stack Tecnol√≥gico

| Componente | Tecnologia | Justificativa |
|------------|------------|---------------|
| **Frontend** | Streamlit | Prototipagem r√°pida, reativo |
| **LLM** | Gemini Flash 1.5 via OpenRouter | R√°pido, gratuito, qualidade |
| **Embeddings** | text-embedding-3-small via OpenRouter | 1536 dims, sem overhead local |
| **Vector DB** | SQLite + NumPy | Simples, port√°vel, leve |
| **Chunking** | RecursiveCharacterTextSplitter | Preserva sem√¢ntica |
| **Cache** | SQLite | Persistente, eficiente |

**Por que OpenRouter?**
- ‚úÖ Unified API (m√∫ltiplos modelos)
- ‚úÖ Fallback autom√°tico
- ‚úÖ Custo otimizado
- ‚úÖ Sem vendor lock-in

---

## 2. Pipeline RAG Detalhado

### 2.1 Fluxo Completo

```
Usu√°rio: "Como o conselho se re√∫ne?"
    ‚Üì
1. SEMANTIC REWRITER
   ‚Üí "Como o conselho se re√∫ne? Qual o quorum? 
      Conforme regimento interno..."
    ‚Üì
2. HyDE (se ativado)
   ‚Üí Gera hip√≥tese: "O Conselho se re√∫ne mediante 
      convoca√ß√£o, conforme Art. 7¬∫..."
    ‚Üì
3. EMBEDDING
   ‚Üí Vetoriza hip√≥tese (ou query)
   ‚Üí [0.23, -0.15, ..., 0.42] (1536 dims)
    ‚Üì
4. VECTOR SEARCH
   ‚Üí Busca top-5 chunks similares
   ‚Üí Filtra por permiss√µes (user_id)
    ‚Üì
5. CONTEXT BUILDING
   ‚Üí Monta prompt com chunks
   ‚Üí Adiciona metadados (fonte, similaridade)
    ‚Üì
6. LLM GENERATION
   ‚Üí Envia para Gemini Flash
   ‚Üí Streaming de resposta
    ‚Üì
7. SOURCE CITATION
   ‚Üí Exibe fontes consultadas
   ‚Üí Percentual de similaridade
```

### 2.2 Semantic Rewriter

**Objetivo**: Expandir queries vagas com contexto

**Exemplo**:
```python
Original: "Qual a pauta?"
Reescrita: "Qual a pauta da pr√≥xima reuni√£o ordin√°ria do CONSUNI? 
            Quais s√£o os itens da ordem do dia? Quando ser√°?"
```

**Implementa√ß√£o**:
```python
# src/agents/semantic_rewriter.py
class SemanticRewriter:
    def enrich(self, query: str, use_llm: bool = True):
        # 1. Heur√≠sticas (r√°pido)
        heuristics = self._apply_heuristics(query)
        
        # 2. LLM (preciso)
        if use_llm:
            llm_expansion = self._llm_expand(query)
            return combine(heuristics, llm_expansion)
        
        return heuristics
```

### 2.3 HyDE (Hypothetical Document Embeddings)

**Conceito**: Gera resposta hipot√©tica e busca por ela

**Vantagem**: Resposta hipot√©tica √© mais similar ao documento real que a query

**Exemplo**:
```
Query: "como o conselho se reune?"
    ‚Üì
An√°lise de Contexto:
  - Conselho: PPGMCC
  - Tipo doc: regimento
  - Formato: "Art. X do Regimento..."
    ‚Üì
Hip√≥tese Gerada (LLM):
  "O Colegiado do PPGMCC se re√∫ne mediante convoca√ß√£o 
   da Coordena√ß√£o ou por requerimento de metade dos 
   membros, conforme Art. 7¬∫ do Regimento Interno..."
    ‚Üì
Embedding da Hip√≥tese:
  [0.45, 0.23, ..., 0.67] (1536 dims)
    ‚Üì
Busca Vetorial:
  Similaridade com Art. 7¬∫: 87% (vs 64% sem HyDE) ‚úÖ
```

**Implementa√ß√£o**:
```python
# src/services/hyde_query_expander.py
class HyDEQueryExpander:
    def expand_query(self, query, history):
        # 1. Analisar contexto
        analysis = self._analyze_context(query, history)
        
        # 2. Gerar hip√≥tese
        hypothesis = self._generate_hypothesis(query, analysis)
        
        # 3. Embeddings
        query_emb = self.embeddings.generate(query)
        hyp_emb = self.embeddings.generate(hypothesis)
        
        return HyDEResult(
            original_query=query,
            hypothetical_answer=hypothesis,
            answer_embedding=hyp_emb,
            confidence=self._calculate_confidence(analysis)
        )
```

**Prompts Domain-Specific**:
```python
# src/utils/hyde_prompts.py
REGIMENTO_HYPOTHESIS_PROMPT = """
Voc√™ est√° gerando resposta hipot√©tica sobre REGIMENTO.

Query: {query}
Conselho: {conselho}

Estrutura t√≠pica:
- "Conforme Art. [n√∫mero] do Regimento [nome]..."
- "O [√≥rg√£o] [a√ß√£o], conforme Art. [n√∫mero]..."

Gere resposta hipot√©tica:
"""
```

### 2.4 Vector Search com Permiss√µes

**Busca com Filtros**:
```python
# src/services/vector_store.py
def search(self, query: str, k: int = 5, user_id: str = None):
    # 1. Gerar embedding
    query_emb = self.embeddings.generate_embedding(query)
    
    # 2. Filtro de permiss√µes
    if user_id:
        permission_filter = """
            WHERE d.is_global = 1 OR d.user_id = ?
        """
        params = (user_id,)
    else:
        permission_filter = "WHERE d.is_global = 1"
        params = ()
    
    # 3. Buscar chunks
    cur.execute(f"""
        SELECT c.conteudo, c.embedding, d.titulo, d.tipo
        FROM chunks c
        JOIN documentos d ON c.documento_id = d.id
        {permission_filter}
    """, params)
    
    # 4. Calcular similaridade
    results = []
    for row in cur.fetchall():
        chunk_emb = np.frombuffer(row['embedding'], dtype=np.float32)
        similarity = cosine_similarity(query_emb, chunk_emb)
        results.append({
            'conteudo': row['conteudo'],
            'similarity': similarity,
            'titulo': row['titulo']
        })
    
    # 5. Ordenar e retornar top-k
    results.sort(key=lambda x: x['similarity'], reverse=True)
    return results[:k]
```

**Similaridade de Cosseno**:
```python
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
```

---

## 3. Embeddings e Vetoriza√ß√£o

### 3.1 Configura√ß√£o OpenRouter

**Modelo**: `openai/text-embedding-3-small`  
**Dimens√µes**: 1536  
**Formato**: float32 (6144 bytes por vetor)

**Por que OpenRouter em vez de local?**

| Aspecto | Local (sentence-transformers) | OpenRouter |
|---------|-------------------------------|------------|
| **RAM** | 2-4GB overhead | 0GB |
| **CPU** | Intensivo (Celeron sofre) | Nenhum |
| **Lat√™ncia** | ~500ms | ~200ms |
| **Qualidade** | Boa (384 dims) | Excelente (1536 dims) |
| **Custo** | Gr√°tis | ~$0.0001/doc |

**Configura√ß√£o**:
```python
# src/services/embeddings.py
class EmbeddingService:
    def __init__(self):
        self.client = OpenAI(
            api_key=config.LLM_API_KEY,
            base_url="https://openrouter.ai/api/v1"
        )
        self.model = "openai/text-embedding-3-small"
    
    def generate_embedding(self, text: str) -> np.ndarray:
        response = self.client.embeddings.create(
            model=self.model,
            input=text
        )
        # IMPORTANTE: Converter para float32!
        embedding = np.array(
            response.data[0].embedding,
            dtype=np.float32  # ‚Üê Essencial!
        )
        return embedding
```

### 3.2 Processamento de Documentos

**Pipeline**:
```
PDF ‚Üí Extra√ß√£o de Texto ‚Üí Chunking ‚Üí Embeddings ‚Üí Armazenamento
```

**Chunking**:
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # ~150-200 palavras
    chunk_overlap=200,      # Evita perda nas bordas
    separators=["\n\n", "\n", ". ", " ", ""]
)

chunks = splitter.split_text(document_text)
```

**Armazenamento**:
```sql
-- Tabela de chunks
CREATE TABLE chunks (
    id INTEGER PRIMARY KEY,
    documento_id INTEGER NOT NULL,
    conteudo TEXT NOT NULL,
    embedding BLOB,           -- 6144 bytes (1536 √ó 4)
    metadata TEXT,
    posicao INTEGER,
    FOREIGN KEY (documento_id) REFERENCES documentos(id)
);

-- Serializa√ß√£o
embedding_blob = embedding.astype(np.float32).tobytes()

-- Deserializa√ß√£o
embedding = np.frombuffer(embedding_blob, dtype=np.float32)
```

---

## 4. Permiss√µes e Seguran√ßa

### 4.1 Sistema de Permiss√µes

**Modelo**:
- **Documentos Globais** (üåç): Vis√≠veis para todos
- **Documentos Privados** (üîí): Vis√≠veis s√≥ para dono

**Schema**:
```sql
CREATE TABLE documentos (
    id INTEGER PRIMARY KEY,
    tipo TEXT NOT NULL,
    titulo TEXT NOT NULL,
    user_id TEXT DEFAULT 'system',  -- Dono
    is_global BOOLEAN DEFAULT 1,    -- Global?
    -- ... outros campos
);
```

**L√≥gica de Busca**:
```python
# Admin v√™ tudo
if user_role == 'admin':
    filter = "1=1"  # Sem filtro

# Usu√°rio comum v√™:
# - Documentos globais
# - Seus documentos privados
else:
    filter = "(is_global = 1 OR user_id = ?)"
    params = (user_id,)
```

### 4.2 Autentica√ß√£o

**PBKDF2** com 100.000 itera√ß√µes:
```python
# src/services/user_service.py
def hash_password(password: str, salt: bytes) -> bytes:
    return hashlib.pbkdf2_hmac(
        'sha256',
        password.encode('utf-8'),
        salt,
        iterations=100000
    )
```

**Roles**:
- `publico`: Acesso b√°sico
- `secs`: Funcionalidades extras
- `admin`: Acesso total

---

## 5. Cache e Performance

### 5.1 Cache Multin√≠vel

**Estrutura**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Cache de Usu√°rio   ‚îÇ  ‚Üê Espec√≠fico do usu√°rio
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Cache Global      ‚îÇ  ‚Üê Compartilhado
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Implementa√ß√£o**:
```python
# src/services/cache_service.py
class CacheService:
    def get(self, query: str, user_id: str):
        # 1. Normalizar query
        normalized = self._normalize(query)
        
        # 2. Buscar em cache de usu√°rio
        user_cache = self._get_user_cache(user_id, normalized)
        if user_cache:
            return user_cache, "user"
        
        # 3. Buscar em cache global
        global_cache = self._get_global_cache(normalized)
        if global_cache:
            return global_cache, "global"
        
        return None, None
    
    def _normalize(self, query: str) -> str:
        # Remove pontua√ß√£o, lowercase, trim
        return query.lower().strip().rstrip('?!.')
```

**M√©tricas**:
- **Hit rate**: ~98%
- **Redu√ß√£o lat√™ncia**: 98% (3s ‚Üí 50ms)
- **Economia API**: 70%

### 5.2 Otimiza√ß√µes para Hardware Modesto

**Configura√ß√£o Recomendada**:
```env
# .env - Otimizado para Celeron N3050
EMBEDDING_PROVIDER=openai  # N√ÉO local!
LLM_MODEL=openrouter/google/gemini-flash-1.5
CACHE_ENABLED=true  # ESSENCIAL!
RAG_TOP_K=5  # N√£o aumentar
```

**Benchmarks** (Celeron N3050, 8GB RAM):

| Opera√ß√£o | Tempo | Observa√ß√£o |
|----------|-------|------------|
| Startup | ~3s | Com cache |
| Query (cache hit) | ~50ms | 98% dos casos |
| Query (cache miss) | ~2-3s | Busca + LLM |
| Upload PDF (10MB) | ~30s | Processamento |
| HyDE query | +500ms | LLM extra |

---

## 6. Agentes Especializados

### 6.1 Focal Agent

**7 Ferramentas**:
1. **Pauta**: Busca pautas de reuni√µes
2. **Ata**: Busca atas
3. **Vota√ß√£o**: Informa√ß√µes sobre vota√ß√µes
4. **Participantes**: Lista de participantes
5. **Resolu√ß√£o**: Busca resolu√ß√µes
6. **Portaria**: Busca portarias
7. **Data**: Informa√ß√µes temporais

**Implementa√ß√£o**:
```python
# src/agents/focal_agent.py
class FocalAgent:
    def run(self, query: str, user_id: str):
        # 1. Detectar ferramenta
        tool = self._detect_tool(query)
        
        # 2. Executar busca especializada
        if tool == 'pauta':
            results = self.vector_store.search_with_filter(
                query,
                filters={'tipo': 'pauta'},
                user_id=user_id
            )
        
        return AgentResult(tool=tool, chunks=results)
```

---

## 7. M√©tricas e Monitoramento

### 7.1 Performance

**Precis√£o RAG**: ~92%  
**Cache hit rate**: ~98%  
**HyDE melhoria**: +20-30%

### 7.2 Capacidade

- **Documentos**: Ilimitado (disco)
- **Chunks**: ~500 tokens m√©dio
- **Embeddings**: 1536 dimens√µes
- **Cache**: 1000 queries (configur√°vel)

